{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cell_Counting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqw4mYU_zKma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkl-i_NYmWXe",
        "colab_type": "text"
      },
      "source": [
        "# All Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot_HEPRImYiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import h5py\n",
        "from numpy.random import RandomState\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import Conv2D, Dropout, Flatten, Dense, Activation, ZeroPadding2D, LeakyReLU, BatchNormalization\n",
        "from keras.layers import Input, Convolution1D, Concatenate, UpSampling2D, Conv2DTranspose\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.activations import relu, softmax\n",
        "from keras import losses, models, optimizers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.utils import data\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WuJOuCPZvsx",
        "colab_type": "text"
      },
      "source": [
        "#Pre-Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNbtVctcIPdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_pad(X, pad):\n",
        "  # input X - python numpy array of shape (m, n_H, n_W) representing a batch of m images\n",
        "  # input pad - integer\n",
        "  # Output X_pad - padded image of shape (m, n_H + 2*pad, n_W + 2*pad)\n",
        "\n",
        "    X_pad = np.zeros([X.shape[0], X.shape[1]+2*pad, X.shape[2]+2*pad])\n",
        "    for i in range(X.shape[0]):\n",
        "      X_pad[i] = np.pad(X[i], pad, mode='constant')\n",
        "    \n",
        "    return X_pad\n",
        "\n",
        "def zero_pad_3d(X, pad):\n",
        "  # input X - python numpy array of shape (m, n_H, n_W,3) representing a batch of m images\n",
        "  # input pad - integer\n",
        "  # Output X_pad - padded image of shape (m, n_H + 2*pad, n_W + 2*pad,3)\n",
        "\n",
        "    X_pad = np.zeros([X.shape[0], X.shape[1]+2*pad, X.shape[2]+2*pad, X.shape[3]])\n",
        "    for i in range(X.shape[0]):\n",
        "      X_pad[i] = np.pad(X[i], ((pad,pad), (pad,pad), (0,0)), mode='constant')\n",
        "    \n",
        "    return X_pad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjeWuqc049ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_total_cnt_no_redundancy(L):\n",
        "  # L is the image having just point annotations\n",
        "  count = np.count_nonzero(L>1)\n",
        "  return count"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BodWKJT85L35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_total_cnt_with_redundancy(T, f=32, stride=1):\n",
        "  # T is the image having redundant counts\n",
        "  # f is the size of the receptive field (the 'r' in the paper)\n",
        "  count = np.sum(T)\n",
        "  count = count/(f*f)\n",
        "  count = count*(stride*stride)\n",
        "  return count"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpviX7kjVTSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_LtoT(L, stride=1, pad=32, f=32):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    L -- numpy array of m images\n",
        "    stride -- stride used\n",
        "    pad -- padding for L\n",
        "    f -- The receptive field size ('r' in the paper)\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conversion output, numpy array of shape (m, n_H, n_W)\n",
        "    \"\"\"\n",
        "     \n",
        "    (m, n_H_prev, n_W_prev) = (L.shape[0] , L.shape[1] , L.shape[2])\n",
        "    n_H = int( (n_H_prev - f + 2*pad)/stride ) \n",
        "    n_W = int( (n_W_prev - f + 2*pad)/stride )\n",
        "    \n",
        "    Z = np.zeros([m,n_H,n_W])\n",
        "    L_pad = zero_pad(L,pad)\n",
        "    \n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        a_pad = L_pad[i]                               # Select ith training example's padded activation\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "               \n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = h*stride\n",
        "                    vert_end = h*stride + f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = w*stride + f\n",
        "                    \n",
        "                    a_slice = a_pad[vert_start:vert_end,horiz_start:horiz_end]\n",
        "                    \n",
        "                    Z[i, h, w] = get_total_cnt_no_redundancy(a_slice)\n",
        "                         \n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W))\n",
        "    return Z"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDy8b13IYY2N",
        "colab_type": "text"
      },
      "source": [
        "# Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE1YErf2TgU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def genGausImage(framesize, mx, my, cov=1):\n",
        "    x, y = np.mgrid[0:framesize, 0:framesize]\n",
        "    pos = np.dstack((x, y))\n",
        "    mean = [mx, my]\n",
        "    cov = [[cov, 0], [0, cov]]\n",
        "    rv = scipy.stats.multivariate_normal(mean, cov).pdf(pos)\n",
        "    # print(rv.shape)\n",
        "    return rv/rv.sum()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5lhB1GjTgpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDensity(height, width, markers, f=32, cov=1):\n",
        "    gaus_img = np.zeros((height,width))\n",
        "    # print(gaus_img.shape)\n",
        "    for k in range(height):\n",
        "        for l in range(width):\n",
        "            if (markers[k,l] > 1):\n",
        "                gaus_img += genGausImage(len(markers),k-f/2,l-f/2,cov)\n",
        "    return gaus_img"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjsxfJ8yText",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_LtoT_gaussian(L, stride=1, pad=32, f=32):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    L -- numpy array of m images\n",
        "    stride -- stride used\n",
        "    pad -- padding for L\n",
        "    f -- The receptive field size ('r' in the paper)\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conversion output, numpy array of shape (m, n_H, n_W)\n",
        "    \"\"\"\n",
        "      \n",
        "    (m, n_H_prev, n_W_prev) = (L.shape[0] , L.shape[1] , L.shape[2])\n",
        "    n_H = int( (n_H_prev - f + 2*pad)/stride ) \n",
        "    n_W = int( (n_W_prev - f + 2*pad)/stride )\n",
        "    \n",
        "    T = np.zeros([m,n_H,n_W])\n",
        "    L_pad = zero_pad(L,pad)\n",
        "    \n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        L_modified = L_pad[i][0:n_H, 0:n_W]\n",
        "        T[i] = getDensity(n_H, n_W, L_modified, f)\n",
        "                                        \n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(T.shape == (m, n_H, n_W))\n",
        "    return T"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjMSNLzrH01P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-Processing Parameters:\n",
        "\n",
        "receptive_field = 32\n",
        "stride = 1\n",
        "cells_kernel = 'square'\n",
        "adipocyte_kernel = 'square'\n",
        "mbm_kernel = 'square'\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGZaVhTWlW1H",
        "colab_type": "text"
      },
      "source": [
        "#CELLS DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xn0Hsjsnaf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"drive/My Drive/countception-master/cells\" \"cells\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KFcExxdmVDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I_data_cells = []\n",
        "L_data_cells = []\n",
        "count_data_cells = []\n",
        "\n",
        "path = 'cells/'\n",
        "\n",
        "for i in range(1,201):\n",
        "  s_I = path + f'{i:03}' + 'cell.png'\n",
        "  s_L = path + f'{i:03}' + 'dots.png' \n",
        "\n",
        "  I = cv2.imread(s_I)\n",
        "  I = I.transpose((2,0,1))\n",
        "  I_data_cells.append(I)\n",
        "\n",
        "  L = cv2.imread(s_L, 0)\n",
        "  L_data_cells.append(L)\n",
        "\n",
        "  count = get_total_cnt_no_redundancy(L)\n",
        "  count_data_cells.append(count)\n",
        "\n",
        "I_data_cells = np.array(I_data_cells)\n",
        "L_data_cells = np.array(L_data_cells)\n",
        "count_data_cells = np.array(count_data_cells)\n",
        "\n",
        "if cells_kernel == 'square':\n",
        "  T_data_cells = conv_LtoT(L_data_cells, stride=stride, f=receptive_field)\n",
        "else:\n",
        "  T_data_cells = conv_LtoT_gaussian(L_data_cells, stride=stride, f=receptive_field)\n",
        "T_data_cells = T_data_cells.reshape(T_data_cells.shape[0], 1, T_data_cells.shape[1], T_data_cells.shape[2])\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4HAJ-9Go54q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(I_data_cells.shape)\n",
        "print(L_data_cells.shape)\n",
        "print(T_data_cells.shape)\n",
        "print(count_data_cells.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yOoi4lFaBNY",
        "colab_type": "text"
      },
      "source": [
        "# ADIPOCYTE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2PLt18laGPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"drive/My Drive/countception-master/adipocyte_data\" \"adipocyte_data\""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8qRYhNPaiPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I_data_adipocyte = []\n",
        "L_data_adipocyte = []\n",
        "count_data_adipocyte = []\n",
        "\n",
        "path_I = 'adipocyte_data/images/'\n",
        "path_L = 'adipocyte_data/annotations/'\n",
        "filenames = []\n",
        "\n",
        "for filename in os.listdir(path_I):\n",
        "  filenames.append(filename)\n",
        "\n",
        "for filename in filenames:\n",
        "  s_I = path_I + filename\n",
        "  s_L = path_L + filename \n",
        "\n",
        "  I = cv2.imread(s_I)\n",
        "  I = I.transpose((2,0,1))\n",
        "  I_data_adipocyte.append(I)\n",
        "\n",
        "  L = cv2.imread(s_L, 0)\n",
        "  L_data_adipocyte.append(L)\n",
        "\n",
        "  count = get_total_cnt_no_redundancy(L)\n",
        "  count_data_adipocyte.append(count)\n",
        "\n",
        "I_data_adipocyte = np.array(I_data_adipocyte)\n",
        "L_data_adipocyte = np.array(L_data_adipocyte)\n",
        "count_data_adipocyte = np.array(count_data_adipocyte)\n",
        "\n",
        "if adipocyte_kernel == 'square':\n",
        "  T_data_adipocyte = conv_LtoT(L_data_adipocyte, stride=stride, f=receptive_field)\n",
        "else:\n",
        "  T_data_adipocyte = conv_LtoT_gaussian(L_data_adipocyte, stride=stride, f=receptive_field)\n",
        "T_data_adipocyte = T_data_adipocyte.reshape(T_data_adipocyte.shape[0], 1, T_data_adipocyte.shape[1], T_data_adipocyte.shape[2])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R2kKg8ccS6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(I_data_adipocyte.shape)\n",
        "print(L_data_adipocyte.shape)\n",
        "print(T_data_adipocyte.shape)\n",
        "print(count_data_adipocyte.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJKuTHTI8_iX",
        "colab_type": "text"
      },
      "source": [
        "#MBM DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz2CTxoK-fGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"drive/My Drive/countception-master/MBM_data\" \"MBM_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy44cjGX-th3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I_data_mbm = []\n",
        "L_data_mbm = []\n",
        "count_data_mbm = []\n",
        "\n",
        "path = 'MBM_data/'\n",
        "filenames_I = []\n",
        "filenames_L = []\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "  if 'dots' in filename:\n",
        "    filenames_L.append(filename)\n",
        "  else:\n",
        "    filenames_I.append(filename)\n",
        "\n",
        "filenames_I.sort()\n",
        "filenames_L.sort()\n",
        "\n",
        "for i in range(len(filenames_I)):\n",
        "  s_I = path + filenames_I[i]\n",
        "  s_L = path + filenames_L[i] \n",
        "\n",
        "  # I = Image.open(s_I)\n",
        "  # # I = I.resize((300, 300))\n",
        "  # I = np.array(I)\n",
        "  # I = I.transpose((2,0,1))\n",
        "  # I_data_mbm.append(I)\n",
        "\n",
        "  I = cv2.imread(s_I)\n",
        "  # I = np.array(Image.fromarray(I, mode='RGB').resize((300,300)))\n",
        "  I = I.transpose((2,0,1))\n",
        "  I_data_mbm.append(I)\n",
        "\n",
        "  # L = Image.open(s_L).convert('L')\n",
        "  # # L = L.resize((300, 300))\n",
        "  # L = np.array(L)\n",
        "  # L_data_mbm.append(L)\n",
        "\n",
        "  L = cv2.imread(s_L, 0)\n",
        "  # L = np.array(Image.fromarray(L).resize((300,300)))\n",
        "  L_data_mbm.append(L)\n",
        "\n",
        "  count = get_total_cnt_no_redundancy(L)\n",
        "  count_data_mbm.append(count)\n",
        "\n",
        "I_data_mbm = np.array(I_data_mbm)\n",
        "L_data_mbm = np.array(L_data_mbm)\n",
        "count_data_mbm = np.array(count_data_mbm)\n",
        "\n",
        "if mbm_kernel == 'square':\n",
        "  T_data_mbm = conv_LtoT(L_data_mbm, stride=stride, f=receptive_field)\n",
        "else:\n",
        "  T_data_mbm = conv_LtoT_gaussian(L_data_mbm, stride=stride, f=receptive_field)\n",
        "T_data_mbm = T_data_mbm.reshape(T_data_mbm.shape[0], 1, T_data_mbm.shape[1], T_data_mbm.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSYXI3zeA4k1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(I_data_mbm.shape)\n",
        "print(L_data_mbm.shape)\n",
        "print(T_data_mbm.shape)\n",
        "print(count_data_mbm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFQQ9bTh8IpP",
        "colab_type": "text"
      },
      "source": [
        "# Train-Test-Val Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH9GRexc8ava",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_val_test_split(I_array, L_array, T_array, count_array, test_data_percent=0.60, val_data_percent=0.20):\n",
        "\n",
        "  number_of_samples = I_array.shape[0]\n",
        "  \n",
        "  index1 = np.random.choice(number_of_samples, math.ceil(test_data_percent*number_of_samples), replace=False)\n",
        "  mask1 = np.zeros(number_of_samples,dtype=bool)\n",
        "  mask1[index1] = True\n",
        "\n",
        "  temp = np.zeros(number_of_samples)\n",
        "  temp = temp[~mask1]\n",
        "\n",
        "  index2 = np.random.choice(temp.shape[0], math.ceil((val_data_percent/(1-test_data_percent))*temp.shape[0]), replace=False)\n",
        "  mask2 = np.zeros(temp.shape[0],dtype=bool)\n",
        "  mask2[index2] = True\n",
        "\n",
        "  I_test = I_array[mask1]\n",
        "  L_test = L_array[mask1]\n",
        "  T_test = T_array[mask1]\n",
        "  count_test = count_array[mask1]\n",
        "\n",
        "  I_train_val = I_array[~mask1]\n",
        "  L_train_val = L_array[~mask1]\n",
        "  T_train_val = T_array[~mask1]\n",
        "  count_train_val = count_array[~mask1]\n",
        "\n",
        "  I_val = I_train_val[mask2]\n",
        "  L_val = L_train_val[mask2]\n",
        "  T_val = T_train_val[mask2]\n",
        "  count_val = count_train_val[mask2]\n",
        "\n",
        "  I_train = I_train_val[~mask2]\n",
        "  L_train = L_train_val[~mask2]\n",
        "  T_train = T_train_val[~mask2]\n",
        "  count_train = count_train_val[~mask2]\n",
        "\n",
        "  final_list = []\n",
        "  final_list.append(I_train)\n",
        "  final_list.append(I_val)\n",
        "  final_list.append(I_test)\n",
        "  final_list.append(L_train)\n",
        "  final_list.append(L_val)\n",
        "  final_list.append(L_test)\n",
        "  final_list.append(T_train)\n",
        "  final_list.append(T_val)\n",
        "  final_list.append(T_test)\n",
        "  final_list.append(count_train)\n",
        "  final_list.append(count_val)\n",
        "  final_list.append(count_test)\n",
        "\n",
        "  return final_list"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_R1Cg-ESLu4",
        "colab_type": "text"
      },
      "source": [
        "CELLS DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEuPncLSBmZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_cells = train_val_test_split(I_data_cells, L_data_cells, T_data_cells, count_data_cells, test_data_percent=0.60, val_data_percent=0.20)\n",
        "\n",
        "I_train_cells = temp_cells[0]\n",
        "I_val_cells = temp_cells[1]\n",
        "I_test_cells = temp_cells[2]\n",
        "L_train_cells = temp_cells[3]\n",
        "L_val_cells = temp_cells[4]\n",
        "L_test_cells = temp_cells[5]\n",
        "T_train_cells = temp_cells[6]\n",
        "T_val_cells = temp_cells[7]\n",
        "T_test_cells = temp_cells[8]\n",
        "count_train_cells = temp_cells[9]\n",
        "count_val_cells = temp_cells[10]\n",
        "count_test_cells = temp_cells[11]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YenBT_QPECQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(I_train_cells.shape, I_val_cells.shape, I_test_cells.shape)\n",
        "print(L_train_cells.shape, L_val_cells.shape, L_test_cells.shape)\n",
        "print(T_train_cells.shape, T_val_cells.shape, T_test_cells.shape)\n",
        "print(count_train_cells.shape, count_val_cells.shape, count_test_cells.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JP8PLqnmMO9",
        "colab_type": "text"
      },
      "source": [
        "ADIPOCYTE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arz09e0QmPhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_adipocyte = train_val_test_split(I_data_adipocyte, L_data_adipocyte, T_data_adipocyte, count_data_adipocyte, test_data_percent=0.60, val_data_percent=0.20)\n",
        "\n",
        "I_train_adipocyte = temp_adipocyte[0]\n",
        "I_val_adipocyte = temp_adipocyte[1]\n",
        "I_test_adipocyte = temp_adipocyte[2]\n",
        "L_train_adipocyte = temp_adipocyte[3]\n",
        "L_val_adipocyte = temp_adipocyte[4]\n",
        "L_test_adipocyte = temp_adipocyte[5]\n",
        "T_train_adipocyte = temp_adipocyte[6]\n",
        "T_val_adipocyte = temp_adipocyte[7]\n",
        "T_test_adipocyte = temp_adipocyte[8]\n",
        "count_train_adipocyte = temp_adipocyte[9]\n",
        "count_val_adipocyte = temp_adipocyte[10]\n",
        "count_test_adipocyte = temp_adipocyte[11]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3HmZfR3mQ_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(I_train_adipocyte.shape, I_val_adipocyte.shape, I_test_adipocyte.shape)\n",
        "print(L_train_adipocyte.shape, L_val_adipocyte.shape, L_test_adipocyte.shape)\n",
        "print(T_train_adipocyte.shape, T_val_adipocyte.shape, T_test_adipocyte.shape)\n",
        "print(count_train_adipocyte.shape, count_val_adipocyte.shape, count_test_adipocyte.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImL6sHPpm8Ir",
        "colab_type": "text"
      },
      "source": [
        "MBM DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFRPvbxkm9Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_mbm = train_val_test_split(I_data_mbm, L_data_mbm, T_data_mbm, count_data_mbm, test_data_percent=0.40, val_data_percent=0.11)\n",
        "\n",
        "I_train_mbm = temp_mbm[0]\n",
        "I_val_mbm = temp_mbm[1]\n",
        "I_test_mbm = temp_mbm[2]\n",
        "L_train_mbm = temp_mbm[3]\n",
        "L_val_mbm = temp_mbm[4]\n",
        "\n",
        "L_test_mbm = temp_mbm[5]\n",
        "T_train_mbm = temp_mbm[6]\n",
        "T_val_mbm = temp_mbm[7]\n",
        "T_test_mbm = temp_mbm[8]\n",
        "count_train_mbm = temp_mbm[9]\n",
        "count_val_mbm = temp_mbm[10]\n",
        "count_test_mbm = temp_mbm[11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlTztlStnADf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(I_train_mbm.shape, I_val_mbm.shape, I_test_mbm.shape)\n",
        "print(L_train_mbm.shape, L_val_mbm.shape, L_test_mbm.shape)\n",
        "print(T_train_mbm.shape, T_val_mbm.shape, T_test_mbm.shape)\n",
        "print(count_train_mbm.shape, count_val_mbm.shape, count_test_mbm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKbqfxNOHo-g",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Self-Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Je339NHudy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Main_Conv_Layer(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=3, stride=1, pad=0, activation=nn.LeakyReLU(0.01)):\n",
        "        super(Main_Conv_Layer, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel_size, stride=stride, padding=pad)\n",
        "        self.activation = activation\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.batch_norm(self.conv1(x)))\n",
        "\n",
        "\n",
        "class Two_Conv(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel_1x1, out_channel_3x3, activation=nn.LeakyReLU(0.01)):\n",
        "        super(Two_Conv, self).__init__()\n",
        "        self.conv1 = Main_Conv_Layer(in_channel, out_channel_1x1, kernel_size=1, pad=0, activation=activation)\n",
        "        self.conv2 = Main_Conv_Layer(in_channel, out_channel_3x3, kernel_size=3, pad=1, activation=activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1_out = self.conv1(x)\n",
        "        conv2_out = self.conv2(x)\n",
        "        output = torch.cat([conv1_out, conv2_out], 1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class ModelCountception(nn.Module):\n",
        "    def __init__(self, in_channel=3, out_channel=1, initial_padding=32):\n",
        "        super(ModelCountception, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.activation = nn.LeakyReLU(0.01)\n",
        "        self.initial_padding = initial_padding\n",
        "\n",
        "        torch.LongTensor()\n",
        "\n",
        "        # layers (comment size for cells dataset)\n",
        "        self.conv1 = Main_Conv_Layer(self.in_channel, 64, kernel_size=3, pad=self.initial_padding, activation=self.activation) #(64,318,318)\n",
        "        self.conv2 = Two_Conv(64, 16, 16, activation=self.activation) #(32,318,318)\n",
        "        self.conv3 = Two_Conv(32, 16, 32, activation=self.activation) #(48,318,318)\n",
        "        self.conv4 = Main_Conv_Layer(48, 16, kernel_size=14, activation=self.activation) #(16,305,305)\n",
        "        self.conv5 = Two_Conv(16, 112, 48, activation=self.activation) #(160,305,305)\n",
        "        self.conv6 = Two_Conv(160, 64, 32, activation=self.activation) #(96,305,305)\n",
        "        self.conv7 = Two_Conv(96, 40, 40, activation=self.activation) #(80,305,305)\n",
        "        self.conv8 = Two_Conv(80, 32, 96, activation=self.activation) #(128,305,305)\n",
        "        self.conv9 = Main_Conv_Layer(128, 32, kernel_size=18, activation=self.activation) #(32,288,288)\n",
        "        self.conv10 = Main_Conv_Layer(32, 64, kernel_size=1, activation=self.activation) #(64,288,288)\n",
        "        self.conv11 = Main_Conv_Layer(64, 64, kernel_size=1, activation=self.activation) #(64,288,288)\n",
        "        self.conv12 = Main_Conv_Layer(64, self.out_channel, kernel_size=1, activation=self.activation) #(1,288,288)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                init.xavier_uniform(m.weight, gain=init.calculate_gain('leaky_relu', param=0.01))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        net = self.conv1(x)\n",
        "        net = self.conv2(net)\n",
        "        net = self.conv3(net)\n",
        "        net = self.conv4(net)\n",
        "        net = self.conv5(net)\n",
        "        net = self.conv6(net)\n",
        "        net = self.conv7(net)\n",
        "        net = self.conv8(net)\n",
        "        net = self.conv9(net)\n",
        "        net = self.conv10(net)\n",
        "        net = self.conv11(net)\n",
        "        net = self.conv12(net)\n",
        "        \n",
        "        return net\n",
        "\n",
        "    def name(self):\n",
        "        return 'countception'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M4A-5e3o4Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_output_count(x_train, y_train, x_val, y_val, x_test, y_test=None, is_print=True, batch_size=4, epochs=1000, lr=0.05, test_batch_size=1, data_type='cells', kernel_type='square'):\n",
        "  if y_test is None:\n",
        "    y_test = np.zeros(1)\n",
        "\n",
        "  x_train, y_train, x_val, y_val, x_test, y_test = map(torch.tensor, (x_train, y_train, x_val, y_val, x_test, y_test))\n",
        "\n",
        "  train_ds = TensorDataset(x_train, y_train)\n",
        "  train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "  val_ds = TensorDataset(x_val, y_val)\n",
        "  val_dl = DataLoader(val_ds, batch_size=batch_size, num_workers=8)\n",
        "\n",
        "  test_ds = TensorDataset(x_test, y_test)\n",
        "  test_dl = DataLoader(test_ds, batch_size=test_batch_size, num_workers=8)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  criterion = nn.L1Loss()\n",
        "  model = ModelCountception().to(device)\n",
        "  solver = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_loss = []\n",
        "    for idx, (inputt, target) in enumerate(train_dl):\n",
        "        inputt = inputt.type(torch.FloatTensor)\n",
        "        target = target.type(torch.FloatTensor)\n",
        "        inputt = inputt.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model.forward(inputt)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Zero grad\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        solver.step()\n",
        "\n",
        "        train_loss.append(loss.data.cpu().numpy())\n",
        "    if is_print:\n",
        "        print(\"Epoch\", epoch, \"- Training Loss:\", np.mean(train_loss), end=',  ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_loss = []\n",
        "        for idx, (inputt, target) in enumerate(val_dl):\n",
        "            inputt = inputt.type(torch.FloatTensor)\n",
        "            target = target.type(torch.FloatTensor)\n",
        "            inputt = inputt.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model.forward(inputt)\n",
        "            val_loss.append(criterion(output, target).data.cpu().numpy())\n",
        "        if is_print:\n",
        "            print(\"Epoch\", epoch, \"- Validation Loss:\", np.mean(val_loss))\n",
        "\n",
        "    if (epoch+1) % 5 == 0 or epoch == epochs-1:\n",
        "        state = {'model_weights': model.state_dict()}\n",
        "        torch.save(state, data_type + \"after_{0}_epochs.model\".format(epoch))\n",
        "\n",
        "  criterion = nn.L1Loss()\n",
        "  model = ModelCountception().to(device)\n",
        "  model.eval()\n",
        "\n",
        "  last = str(epochs-1)\n",
        "  from_before = torch.load(data_type + 'after_' + last + '_epochs.model')\n",
        "  model_weights = from_before['model_weights']\n",
        "  model.load_state_dict(model_weights)\n",
        "\n",
        "\n",
        "  predicted_T = []\n",
        "\n",
        "  test_loss = []\n",
        "  with torch.no_grad():\n",
        "      for idx, (inputt, target) in enumerate(test_dl):\n",
        "          inputt = inputt.type(torch.FloatTensor)\n",
        "          target = target.type(torch.FloatTensor)\n",
        "          inputt = inputt.to(device)\n",
        "          target = target.to(device)\n",
        "          output = model.forward(inputt)\n",
        "          test_loss.append(criterion(output, target).data.cpu().numpy())\n",
        "\n",
        "          output = output.cpu().numpy()\n",
        "          for i in range(output.shape[0]):\n",
        "            predicted_T.append(output[i])\n",
        "\n",
        "      if is_print:\n",
        "          print('MAE of Test Set: ', np.mean(test_loss))\n",
        "\n",
        "  actual_cnt = []\n",
        "  predicted_cnt = []\n",
        "  cnt_loss = []\n",
        "  y_test = y_test.cpu().numpy()\n",
        "\n",
        "  for i in range(y_test.shape[0]):\n",
        "    if kernel_type == 'square':\n",
        "      cnt1 = get_total_cnt_with_redundancy(y_test[i])\n",
        "      cnt2 = get_total_cnt_with_redundancy(predicted_T[i])\n",
        "    else:\n",
        "      cnt1 = get_total_cnt_with_redundancy(y_test[i], f=1)\n",
        "      cnt2 = get_total_cnt_with_redundancy(predicted_T[i], f=1)\n",
        "    \n",
        "    actual_cnt.append(cnt1)\n",
        "    predicted_cnt.append(cnt2)\n",
        "    cnt_loss.append(abs(cnt1-cnt2))\n",
        "\n",
        "    if is_print:\n",
        "        print(cnt1,cnt2)\n",
        "\n",
        "  if is_print:\n",
        "      print('Mean Difference in Counts', np.mean(cnt_loss))\n",
        "\n",
        "  predicted_cnt = np.array(predicted_cnt)\n",
        "  predicted_T = np.array(predicted_T)\n",
        "  return predicted_cnt, predicted_T\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T3wnMT3sxni",
        "colab_type": "text"
      },
      "source": [
        "CELLS DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPpiQ6cmq31G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Parameters:\n",
        "batch_size = 2\n",
        "epochs = 300\n",
        "lr = 0.001\n",
        "\n",
        "prediction_cnt_cells, prediction_T_cells = get_output_count(I_train_cells, T_train_cells, I_val_cells, T_val_cells, I_test_cells, T_test_cells, batch_size=batch_size, epochs=epochs, lr=lr, test_batch_size=2, data_type='cells', kernel_type=cells_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAv9s6ccQwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_predicted = prediction_T_cells[31]\n",
        "temp = im_predicted.reshape(im_predicted.shape[1], im_predicted.shape[2])\n",
        "im1 = plt.imshow(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2zbhjmdG7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = T_test_cells[31]\n",
        "temp2 = im.reshape(im.shape[1], im.shape[2])\n",
        "im2 = plt.imshow(temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLqKzxhqszS2",
        "colab_type": "text"
      },
      "source": [
        "ADIPOCYTE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCPWBNtnsvRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Parameters:\n",
        "batch_size = 2\n",
        "epochs = 500\n",
        "lr = 0.005\n",
        "\n",
        "prediction_adipocyte, prediction_T_adipocyte = get_output_count(I_train_adipocyte, T_train_adipocyte, I_val_adipocyte, T_val_adipocyte, I_test_adipocyte, T_test_adipocyte, batch_size=batch_size, epochs=epochs, lr=lr, test_batch_size=1, data_type='adipocyte', kernel_type=adipocyte_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dQ2f2p-scp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_predicted = prediction_T_adipocyte[-16]\n",
        "temp = im_predicted.reshape(im_predicted.shape[1], im_predicted.shape[2])\n",
        "im1 = plt.imshow(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_lMfp8S-o4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = T_test_adipocyte[-16]\n",
        "temp2 = im.reshape(im.shape[1], im.shape[2])\n",
        "im2 = plt.imshow(temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rcMPtnHs2_D",
        "colab_type": "text"
      },
      "source": [
        "MBM DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xzBRpk4sww7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Training Parameters:\n",
        "batch_size = 2\n",
        "epochs = 400\n",
        "lr = 0.005\n",
        "\n",
        "prediction_mbm, prediction_T_mbm = get_output_count(I_train_mbm, T_train_mbm, I_val_mbm, T_val_mbm, I_test_mbm, T_test_mbm, batch_size=batch_size, epochs=epochs, lr=lr, test_batch_size=1, data_type='mbm', kernel_type=mbm_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB_uHkWY7Lbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_predicted = prediction_T_mbm[10]\n",
        "temp = im_predicted.reshape(im_predicted.shape[1], im_predicted.shape[2])\n",
        "im1 = plt.imshow(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP-gZ9Pq7O6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = T_test_mbm[10]\n",
        "temp2 = im.reshape(im.shape[1], im.shape[2])\n",
        "im2 = plt.imshow(temp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxBNWCrQFpWO",
        "colab_type": "text"
      },
      "source": [
        "# Convolution Model Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFWmVq0DFvcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def conv_model(x_train, y_train, x_val, y_val, x_test, initial_padding=32):\n",
        "  \n",
        "#   # input_img = Input(shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])) #(256,256,3)\n",
        "#   input_img = Input(shape = (256, 256, 3)) #(256,256,3)\n",
        "  \n",
        "#   pad = ZeroPadding2D(initial_padding)(input_img) #(320,320,3)\n",
        "#   conv1 = Conv2D(64, (3,3), kernel_initializer=\"glorot_uniform\",)(pad)\n",
        "#   conv1 = LeakyReLU()(conv1)\n",
        "#   conv1 = BatchNormalization()(conv1) #(318,318,64)\n",
        "\n",
        "#   conv2a = Conv2D(16, (1,1), kernel_initializer=\"glorot_uniform\",)(conv1)\n",
        "#   conv2a = LeakyReLU()(conv2a)\n",
        "#   conv2a = BatchNormalization()(conv2a)\n",
        "#   conv2b = ZeroPadding2D(1)(conv1)\n",
        "#   conv2b = Conv2D(16, (3,3), kernel_initializer=\"glorot_uniform\",)(conv2b)\n",
        "#   conv2b = LeakyReLU()(conv2b)\n",
        "#   conv2b = BatchNormalization()(conv2b)\n",
        "#   conv2 = Concatenate()([conv2a, conv2b]) #(318,318,32)\n",
        "\n",
        "#   conv3a = Conv2D(16, (1,1), kernel_initializer=\"glorot_uniform\",)(conv2)\n",
        "#   conv3a = LeakyReLU()(conv3a)\n",
        "#   conv3a = BatchNormalization()(conv3a)\n",
        "#   conv3b = ZeroPadding2D(1)(conv2)\n",
        "#   conv3b = Conv2D(32, (3,3), kernel_initializer=\"glorot_uniform\",)(conv3b)\n",
        "#   conv3b = LeakyReLU()(conv3b)\n",
        "#   conv3b = BatchNormalization()(conv3b)\n",
        "#   conv3 = Concatenate()([conv3a, conv3b]) #(318,318,48)\n",
        "\n",
        "#   conv4 = Conv2D(16, (14,14), kernel_initializer=\"glorot_uniform\",)(conv3)\n",
        "#   conv4 = LeakyReLU()(conv4)\n",
        "#   conv4 = BatchNormalization()(conv4) #(305,305,16)\n",
        "\n",
        "#   conv5a = Conv2D(112, (1,1), kernel_initializer=\"glorot_uniform\",)(conv4)\n",
        "#   conv5a = LeakyReLU()(conv5a)\n",
        "#   conv5a = BatchNormalization()(conv5a)\n",
        "#   conv5b = ZeroPadding2D(1)(conv4)\n",
        "#   conv5b = Conv2D(48, (3,3), kernel_initializer=\"glorot_uniform\",)(conv5b)\n",
        "#   conv5b = LeakyReLU()(conv5b)\n",
        "#   conv5b = BatchNormalization()(conv5b)\n",
        "#   conv5 = Concatenate()([conv5a, conv5b]) #(305,305,160)\n",
        "\n",
        "#   conv6a = Conv2D(64, (1,1), kernel_initializer=\"glorot_uniform\",)(conv5)\n",
        "#   conv6a = LeakyReLU()(conv6a)\n",
        "#   conv6a = BatchNormalization()(conv6a)\n",
        "#   conv6b = ZeroPadding2D(1)(conv5)\n",
        "#   conv6b = Conv2D(32, (3,3), kernel_initializer=\"glorot_uniform\",)(conv6b)\n",
        "#   conv6b = LeakyReLU()(conv6b)\n",
        "#   conv6b = BatchNormalization()(conv6b)\n",
        "#   conv6 = Concatenate()([conv6a, conv6b]) #(305,305,96)\n",
        "\n",
        "#   conv7a = Conv2D(40, (1,1), kernel_initializer=\"glorot_uniform\",)(conv6)\n",
        "#   conv7a = LeakyReLU()(conv7a)\n",
        "#   conv7a = BatchNormalization()(conv7a)\n",
        "#   conv7b = ZeroPadding2D(1)(conv6)\n",
        "#   conv7b = Conv2D(40, (3,3), kernel_initializer=\"glorot_uniform\",)(conv7b)\n",
        "#   conv7b = LeakyReLU()(conv7b)\n",
        "#   conv7b = BatchNormalization()(conv7b)\n",
        "#   conv7 = Concatenate()([conv7a, conv7b]) #(305,305,80)\n",
        "\n",
        "#   conv8a = Conv2D(32, (1,1), kernel_initializer=\"glorot_uniform\",)(conv7)\n",
        "#   conv8a = LeakyReLU()(conv8a)\n",
        "#   conv8a = BatchNormalization()(conv8a)\n",
        "#   conv8b = ZeroPadding2D(1)(conv7)\n",
        "#   conv8b = Conv2D(96, (3,3), kernel_initializer=\"glorot_uniform\",)(conv8b)\n",
        "#   conv8b = LeakyReLU()(conv8b)\n",
        "#   conv8b = BatchNormalization()(conv8b)\n",
        "#   conv8 = Concatenate()([conv8a, conv8b]) #(305,305,128)\n",
        "\n",
        "#   conv9 = Conv2D(32, (18,18), kernel_initializer=\"glorot_uniform\",)(conv8)\n",
        "#   conv9 = LeakyReLU()(conv9)\n",
        "#   conv9 = BatchNormalization()(conv9) #(288,288,32)\n",
        "\n",
        "#   conv10 = Conv2D(64, (1,1), kernel_initializer=\"glorot_uniform\",)(conv9)\n",
        "#   conv10 = LeakyReLU()(conv10)\n",
        "#   conv10 = BatchNormalization()(conv10) #(288,288,64)\n",
        "\n",
        "#   conv11 = Conv2D(64, (1,1), kernel_initializer=\"glorot_uniform\",)(conv10)\n",
        "#   conv11 = LeakyReLU()(conv11)\n",
        "#   conv11 = BatchNormalization()(conv11) #(288,288,64)\n",
        "\n",
        "#   conv12 = Conv2D(1, (1,1), kernel_initializer=\"glorot_uniform\",)(conv11)\n",
        "#   conv12 = LeakyReLU()(conv12)\n",
        "#   conv12 = BatchNormalization()(conv12) #(288,288,1)\n",
        "\n",
        "#   # out = Flatten()(conv12)\n",
        "\n",
        "#   model = models.Model(inputs=input_img, outputs=conv12)\n",
        "#   opt = optimizers.Adam(0.001)\n",
        "#   model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=opt)\n",
        "\n",
        "#   # model.summary()\n",
        "\n",
        "#   model.fit(x_train, y_train, batch_size=1, epochs=5, verbose=1, validation_data=(x_val, y_val))\n",
        "\n",
        "#   # model.summary()\n",
        "\n",
        "#   # model.fit(x=x_train, y=y_train, batch_size=1, epochs=1, verbose=1, validation_data=(x_val, y_val))\n",
        "#   # # model.fit(x=x_train, y=y_train, batch_size=1, epochs=1, verbose=1)\n",
        "#   #   model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
        "\n",
        "#   y_pred = model.predict(x_test)\n",
        "\n",
        "#   tf.keras.backend.clear_session()\n",
        "#   del model\n",
        "\n",
        "#   return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgg2yZ9JjgaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train = []\n",
        "# for i in range(0,I_train_cells.shape[0],2):\n",
        "#   I1 = I_train_cells[i]\n",
        "#   I2 = I_train_cells[i+1]\n",
        "\n",
        "#   x_train.append((I1,I2))\n",
        "\n",
        "# x_train = np.array(x_train)\n",
        "# print(x_train.shape)\n",
        "\n",
        "# x_val = []\n",
        "# for i in range(0,I_val_cells.shape[0],2):\n",
        "#   I1 = I_val_cells[i]\n",
        "#   I2 = I_val_cells[i+1]\n",
        "\n",
        "#   x_val.append((I1,I2))\n",
        "\n",
        "# x_val = np.array(x_val)\n",
        "# print(x_val.shape)\n",
        "\n",
        "# x_test = []\n",
        "# for i in range(0,I_test_cells.shape[0],2):\n",
        "#   I1 = I_test_cells[i]\n",
        "#   I2 = I_test_cells[i+1]\n",
        "\n",
        "#   x_test.append((I1,I2))\n",
        "\n",
        "# x_test = np.array(x_test)\n",
        "# print(x_test.shape)\n",
        "\n",
        "\n",
        "# y_train = []\n",
        "# for i in range(0,T_train_cells.shape[0],2):\n",
        "#   T1 = T_train_cells[i]\n",
        "#   T2 = T_train_cells[i+1]\n",
        "\n",
        "#   T1 = T1.reshape(T1.shape[0], T1.shape[1], 1)\n",
        "#   T2 = T2.reshape(T2.shape[0], T2.shape[1], 1)\n",
        "\n",
        "#   y_train.append((T1,T2))\n",
        "\n",
        "# y_train = np.array(y_train)\n",
        "# print(y_train.shape)\n",
        "\n",
        "# y_val = []\n",
        "# for i in range(0,T_val_cells.shape[0],2):\n",
        "#   T1 = T_val_cells[i]\n",
        "#   T2 = T_val_cells[i+1]\n",
        "\n",
        "#   T1 = T1.reshape(T1.shape[0], T1.shape[1], 1)\n",
        "#   T2 = T2.reshape(T2.shape[0], T2.shape[1], 1)\n",
        "\n",
        "#   y_val.append((T1,T2))\n",
        "\n",
        "# y_val = np.array(y_val)\n",
        "# print(y_val.shape)\n",
        "\n",
        "# y_test = []\n",
        "# for i in range(0,T_test_cells.shape[0],2):\n",
        "#   T1 = T_test_cells[i]\n",
        "#   T2 = T_test_cells[i+1]\n",
        "\n",
        "#   T1 = T1.reshape(T1.shape[0], T1.shape[1], 1)\n",
        "#   T2 = T2.reshape(T2.shape[0], T2.shape[1], 1)\n",
        "\n",
        "#   y_test.append((T1,T2))\n",
        "\n",
        "# y_test = np.array(y_test)\n",
        "# print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cukyIsgJpE6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# T_train_cells = T_train_cells.reshape(T_train_cells.shape[0], T_train_cells.shape[1], T_train_cells.shape[2], 1)\n",
        "# T_val_cells = T_val_cells.reshape(T_val_cells.shape[0], T_val_cells.shape[1], T_val_cells.shape[2], 1)\n",
        "\n",
        "# F_I = conv_model(I_train_cells, T_train_cells, I_val_cells, T_val_cells, I_test_cells)\n",
        "\n",
        "# # F_I = conv_model(x_train, y_train, x_val, y_val, x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egnRkk87sz6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # print(F_I)\n",
        "# cnt_loss = []\n",
        "# for i in range(F_I.shape[0]):\n",
        "#   # print(get_total_cnt_with_redundancy(F_I[i]))\n",
        "#   # print(get_total_cnt_with_redundancy(T_test_cells[i]))\n",
        "#   cnt_loss.append(abs(get_total_cnt_with_redundancy(F_I[i]) - get_total_cnt_with_redundancy(T_test_cells[i])))\n",
        "\n",
        "# print(np.mean(cnt_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3xv1Z8psrfs",
        "colab_type": "text"
      },
      "source": [
        "#Keras with 32*32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQL9_-H2s0yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def conv_model_32(x_train, y_train, x_val, y_val, x_test):\n",
        "  \n",
        "#   # input_img = Input(shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])) #(256,256,3)\n",
        "#   input_img = Input(shape = (32, 32, 3))\n",
        "  \n",
        "#   # pad = ZeroPadding2D(initial_padding)(input_img) #(32,32,3)\n",
        "#   conv1 = Conv2D(64, (3,3), kernel_initializer=\"glorot_uniform\",)(input_img)\n",
        "#   conv1 = LeakyReLU()(conv1)\n",
        "#   conv1 = BatchNormalization()(conv1) #(30,30,64)\n",
        "\n",
        "#   conv2a = Conv2D(16, (1,1), kernel_initializer=\"glorot_uniform\",)(conv1)\n",
        "#   conv2a = LeakyReLU()(conv2a)\n",
        "#   conv2a = BatchNormalization()(conv2a)\n",
        "#   conv2b = ZeroPadding2D(1)(conv1)\n",
        "#   conv2b = Conv2D(16, (3,3), kernel_initializer=\"glorot_uniform\",)(conv2b)\n",
        "#   conv2b = LeakyReLU()(conv2b)\n",
        "#   conv2b = BatchNormalization()(conv2b)\n",
        "#   conv2 = Concatenate()([conv2a, conv2b]) #(30,30,32)\n",
        "\n",
        "#   conv3a = Conv2D(16, (1,1), kernel_initializer=\"glorot_uniform\",)(conv2)\n",
        "#   conv3a = LeakyReLU()(conv3a)\n",
        "#   conv3a = BatchNormalization()(conv3a)\n",
        "#   conv3b = ZeroPadding2D(1)(conv2)\n",
        "#   conv3b = Conv2D(32, (3,3), kernel_initializer=\"glorot_uniform\",)(conv3b)\n",
        "#   conv3b = LeakyReLU()(conv3b)\n",
        "#   conv3b = BatchNormalization()(conv3b)\n",
        "#   conv3 = Concatenate()([conv3a, conv3b]) #(30,30,48)\n",
        "\n",
        "#   conv4 = Conv2D(16, (14,14), kernel_initializer=\"glorot_uniform\",)(conv3)\n",
        "#   conv4 = LeakyReLU()(conv4)\n",
        "#   conv4 = BatchNormalization()(conv4) #(17,17,16)\n",
        "\n",
        "#   conv5a = Conv2D(112, (1,1), kernel_initializer=\"glorot_uniform\",)(conv4)\n",
        "#   conv5a = LeakyReLU()(conv5a)\n",
        "#   conv5a = BatchNormalization()(conv5a)\n",
        "#   conv5b = ZeroPadding2D(1)(conv4)\n",
        "#   conv5b = Conv2D(48, (3,3), kernel_initializer=\"glorot_uniform\",)(conv5b)\n",
        "#   conv5b = LeakyReLU()(conv5b)\n",
        "#   conv5b = BatchNormalization()(conv5b)\n",
        "#   conv5 = Concatenate()([conv5a, conv5b]) #(17,17,160)\n",
        "\n",
        "#   conv6a = Conv2D(64, (1,1), kernel_initializer=\"glorot_uniform\",)(conv5)\n",
        "#   conv6a = LeakyReLU()(conv6a)\n",
        "#   conv6a = BatchNormalization()(conv6a)\n",
        "#   conv6b = ZeroPadding2D(1)(conv5)\n",
        "#   conv6b = Conv2D(32, (3,3), kernel_initializer=\"glorot_uniform\",)(conv6b)\n",
        "#   conv6b = LeakyReLU()(conv6b)\n",
        "#   conv6b = BatchNormalization()(conv6b)\n",
        "#   conv6 = Concatenate()([conv6a, conv6b]) #(17,17,96)\n",
        "\n",
        "#   conv7a = Conv2D(40, (1,1), kernel_initializer=\"glorot_uniform\",)(conv6)\n",
        "#   conv7a = LeakyReLU()(conv7a)\n",
        "#   conv7a = BatchNormalization()(conv7a)\n",
        "#   conv7b = ZeroPadding2D(1)(conv6)\n",
        "#   conv7b = Conv2D(40, (3,3), kernel_initializer=\"glorot_uniform\",)(conv7b)\n",
        "#   conv7b = LeakyReLU()(conv7b)\n",
        "#   conv7b = BatchNormalization()(conv7b)\n",
        "#   conv7 = Concatenate()([conv7a, conv7b]) #(17,17,80)\n",
        "\n",
        "#   conv8a = Conv2D(32, (1,1), kernel_initializer=\"glorot_uniform\",)(conv7)\n",
        "#   conv8a = LeakyReLU()(conv8a)\n",
        "#   conv8a = BatchNormalization()(conv8a)\n",
        "#   conv8b = ZeroPadding2D(1)(conv7)\n",
        "#   conv8b = Conv2D(96, (3,3), kernel_initializer=\"glorot_uniform\",)(conv8b)\n",
        "#   conv8b = LeakyReLU()(conv8b)\n",
        "#   conv8b = BatchNormalization()(conv8b)\n",
        "#   conv8 = Concatenate()([conv8a, conv8b]) #(17,17,128)\n",
        "\n",
        "#   conv9 = Conv2D(32, (17,17), kernel_initializer=\"glorot_uniform\",)(conv8)\n",
        "#   conv9 = LeakyReLU()(conv9)\n",
        "#   conv9 = BatchNormalization()(conv9) #(1,1,32)\n",
        "\n",
        "#   conv10 = Conv2D(64, (1,1), kernel_initializer=\"glorot_uniform\",)(conv9)\n",
        "#   conv10 = LeakyReLU()(conv10)\n",
        "#   conv10 = BatchNormalization()(conv10) #(1,1,64)\n",
        "\n",
        "#   conv11 = Conv2D(64, (1,1), kernel_initializer=\"glorot_uniform\",)(conv10)\n",
        "#   conv11 = LeakyReLU()(conv11)\n",
        "#   conv11 = BatchNormalization()(conv11) #(1,1,64)\n",
        "\n",
        "#   conv12 = Conv2D(1, (1,1), kernel_initializer=\"glorot_uniform\",)(conv11)\n",
        "#   conv12 = LeakyReLU()(conv12)\n",
        "#   conv12 = BatchNormalization()(conv12) #(1,1,1)\n",
        "\n",
        "#   # out = Flatten()(conv12)\n",
        "\n",
        "#   model = models.Model(inputs=input_img, outputs=conv12)\n",
        "#   opt = optimizers.Adam(0.005)\n",
        "#   model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=opt)\n",
        "\n",
        "#   # model.summary()\n",
        "\n",
        "#   model.fit(x_train, y_train, batch_size=4, epochs=100, verbose=1, validation_data=(x_val, y_val))\n",
        "\n",
        "#   # model.summary()\n",
        "\n",
        "#   # model.fit(x=x_train, y=y_train, batch_size=1, epochs=1, verbose=1, validation_data=(x_val, y_val))\n",
        "#   # # model.fit(x=x_train, y=y_train, batch_size=1, epochs=1, verbose=1)\n",
        "#   #   model.compile(loss=tf.keras.losses.MeanAbsoluteError(), optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])\n",
        "\n",
        "#   y_pred = model.predict(x_test)\n",
        "\n",
        "#   tf.keras.backend.clear_session()\n",
        "#   del model\n",
        "\n",
        "#   return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K62tc6-wZsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I_train_cells_32 = zero_pad_3d(I_train_cells, 32)\n",
        "# I_val_cells_32 = zero_pad_3d(I_val_cells, 32)\n",
        "# I_test_cells_32 = zero_pad_3d(I_test_cells, 32)\n",
        "\n",
        "# L_train_cells_32 = zero_pad(L_train_cells,32)\n",
        "# L_val_cells_32 = zero_pad(L_val_cells,32)\n",
        "# L_test_cells_32 = zero_pad(L_test_cells,32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7jC7RuxtMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def conv_to_32(L, stride=1, f=32, is_y=False):\n",
        "\n",
        "#     # Retrieve dimensions from L's shape (≈1 line)  \n",
        "#     # (m, n_H_prev, n_W_prev, n_D_prev) = (L.shape[0] , L.shape[1] , L.shape[2], L.shape[3])\n",
        "    \n",
        "#     # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
        "#     # n_H = int( (n_H_prev - f + 1)/stride ) \n",
        "#     # n_W = int( (n_W_prev - f + 1)/stride )\n",
        "\n",
        "#     # n_H = int(n_H_prev/f)\n",
        "#     # n_W = int(n_W_prev/f)\n",
        "    \n",
        "#     # Initialize the output volume Z with zeros. (≈1 line)\n",
        "#     if is_y:\n",
        "#       (m, n_H_prev, n_W_prev) = (L.shape[0] , L.shape[1] , L.shape[2])\n",
        "#       n_H = int(n_H_prev/f)\n",
        "#       n_W = int(n_W_prev/f)\n",
        "#       Z = np.zeros([m*n_H*n_W,1])\n",
        "#     else:\n",
        "#       (m, n_H_prev, n_W_prev, n_D_prev) = (L.shape[0] , L.shape[1] , L.shape[2], L.shape[3])\n",
        "#       n_H = int(n_H_prev/f)\n",
        "#       n_W = int(n_W_prev/f)\n",
        "#       Z = np.zeros([m*n_H*n_W, f, f, n_D_prev])\n",
        "#     # Z = []\n",
        "    \n",
        "#     # Create L_pad by padding L\n",
        "#     # L_pad = zero_pad(L,pad)\n",
        "#     cnt = 0\n",
        "    \n",
        "#     for i in range(m):                               # loop over the batch of training examples\n",
        "#         a_prev = L[i]                               # Select ith training example's padded activation\n",
        "#         for h in range(n_H):                           # loop over vertical axis of the output volume\n",
        "#             for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "#                 # for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    \n",
        "#                     # Find the corners of the current \"slice\" (≈4 lines)\n",
        "#                     vert_start = h*f\n",
        "#                     vert_end = h*f + f\n",
        "#                     horiz_start = w*f\n",
        "#                     horiz_end = w*f + f\n",
        "                    \n",
        "#                     # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
        "#                     # a_slice_prev = a_prev[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "                    \n",
        "#                     # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
        "#                     if is_y:\n",
        "#                       a_slice_prev = a_prev[vert_start:vert_end,horiz_start:horiz_end]\n",
        "#                       Z[cnt] = get_total_cnt_no_redundancy(a_slice_prev)\n",
        "#                     else:\n",
        "#                       a_slice_prev = a_prev[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "#                       Z[cnt] = a_slice_prev\n",
        "#                     cnt += 1\n",
        "#                     # Z.append(a_slice_prev)\n",
        "\n",
        "    \n",
        "#     # Making sure your output shape is correct\n",
        "#     # assert(Z.shape == (m, n_H, n_W))\n",
        "    \n",
        "#     # Save information in \"cache\" for the backprop\n",
        "#     # cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "#     # Z = np.array(Z)\n",
        "#     # assert(Z.shape == (m*n_H*n_W, 32, 32, n_D_prev))\n",
        "#     return Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KDSbEx8zsQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train_32 = conv_to_32(I_train_cells_32)\n",
        "# x_val_32 = conv_to_32(I_val_cells_32)\n",
        "# x_test_32 = conv_to_32(I_test_cells_32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZPmG8J55Hky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_train_32 = conv_to_32(L_train_cells_32, is_y=True)\n",
        "# y_val_32 = conv_to_32(L_val_cells_32, is_y=True)\n",
        "# y_test_32 = conv_to_32(L_test_cells_32, is_y=True)\n",
        "\n",
        "# y_train_32 = y_train_32.reshape(y_train_32.shape[0],1,1)\n",
        "# y_val_32 = y_val_32.reshape(y_val_32.shape[0],1,1)\n",
        "# y_test_32 = y_test_32.reshape(y_test_32.shape[0],1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbot4CKU71G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(x_train_32.shape)\n",
        "# print(y_train_32.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTF-TBsG8KbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = conv_model_32(x_train_32, y_train_32, x_val_32, y_val_32, x_test_32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vmFpHSnAu0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(x_test_32.shape[0]):\n",
        "#   print(x[i], y_test_32[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}